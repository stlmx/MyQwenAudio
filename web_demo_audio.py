from utils import detect_platform
platform = detect_platform()

if platform == "huawei":
    import os
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"

    import torch
    import torch_npu
    from torch_npu.contrib import transfer_to_npu


elif platform == "nvidia":
    import torch

else:
    raise EnvironmentError("Unsupported platform: cannot import torch.")

import base64
import copy
import gradio as gr
import os
import re
import secrets
import tempfile
from argparse import ArgumentParser
from pathlib import Path
from pydub import AudioSegment
from PIL import Image

# ä½ è‡ªå·±çš„æ¨¡å‹ä¸tokenizerå¼•ç”¨
from transformers import GenerationConfig
from modeling_qwen_dev import QWenLMHeadOmniModel
from tokenization_qwen import QWenTokenizer

DEFAULT_CKPT_PATH = "/home/ma-user/work/lmx/codes/MyQwenAudio/save/qwen-omni-chat_transmission_line_hongwai_lr2e-4_20250110"

def _get_args():
    parser = ArgumentParser()
    parser.add_argument("-c", "--checkpoint-path", type=str, default=DEFAULT_CKPT_PATH,
                        help="Checkpoint name or path, default to %(default)r")
    parser.add_argument("--cpu-only", action="store_true", help="Run demo with CPU only")
    parser.add_argument("--share", action="store_true", default=False,
                        help="Create a publicly shareable link for the interface.")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--server-port", type=int, default=6006,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="127.0.0.1",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def _load_model_tokenizer(args):
    tokenizer = QWenTokenizer.from_pretrained(
        args.checkpoint_path,
        padding_side="right",
        use_fast=False,
    )

    if args.cpu_only:
        device_map = "cpu"
    else:
        # ä¹Ÿå¯ä½¿ç”¨ cuda:n æ¥æŒ‡å®šGPUï¼Œæˆ– "auto"
        device_map = "cuda"

    model = QWenLMHeadOmniModel.from_pretrained(
        args.checkpoint_path,
        device_map=device_map,
        trust_remote_code=True,
        resume_download=True,
        torch_dtype=torch.float16
    ).eval()

    model.generation_config = GenerationConfig.from_pretrained(
        args.checkpoint_path, trust_remote_code=True, resume_download=True,
    )

    return model, tokenizer


def _parse_text(text):
    """
    ä»…ç”¨äºå‰ç«¯å±•ç¤ºæ—¶ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œç®€å•çš„ä»£ç å—è§£æå’Œè½¬ä¹‰å¤„ç†ï¼Œ
    é˜²æ­¢markdown/htmlä»£ç ç‰‡æ®µæ±¡æŸ“UIã€‚
    """
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f"<br></code></pre>"
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", r"\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>" + line
    text = "".join(lines)
    return text


def _launch_demo(args, model, tokenizer):
    uploaded_file_dir = os.environ.get("GRADIO_TEMP_DIR") or str(
        Path(tempfile.gettempdir()) / "gradio"
    )

    def predict(_chatbot, task_history):
        """
        æ ¸å¿ƒèŠå¤©å‡½æ•°ï¼š
        1. ä» task_history ä¸­æå–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥ query
        2. å°†æ‰€æœ‰ï¼ˆå†å² + å½“å‰ï¼‰ç”¨æˆ·è¾“å…¥æ‹¼æ¥ä¸ºè®­ç»ƒæ—¶å¯¹åº”çš„å¤šæ¨¡æ€æ–‡æœ¬æ ¼å¼ 
           (Picture x: <img>...</img>, Audio x: <audio>...</audio>, æ–‡å­—ç­‰)
        3. è°ƒç”¨ model.chat(...) å¾—åˆ°å›å¤
        4. æ›´æ–°å‰ç«¯æ˜¾ç¤º
        """
        # å–åˆ°æœ€æ–°ä¸€æ¡ (q, a)
        # å…¶ä¸­ q å¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€tuple(éŸ³é¢‘) æˆ– dict(å›¾ç‰‡)
        query = task_history[-1][0]  
        
        # ====== å°†å†å²å¯¹è¯+æœ€æ–°query æ‹¼æˆ "history, message" ======
        # 1) æ·±æ‹·è´ä¸€ä¸‹ï¼Œé˜²æ­¢è¾¹éå†è¾¹æ”¹
        history_cp = copy.deepcopy(task_history)

        pre = ""             # ç”¨äºæ‹¼æ¥å¤šæ¨¡æ€prompt
        history_filter = []  # æ”¶é›† (prompt, reply)
        audio_idx = 1
        image_idx = 1
        
        for i, (q, a) in enumerate(history_cp):
            # æŒ‰ç±»å‹æ‹¼è£…
            if isinstance(q, str):
                # æ–‡æœ¬è¾“å…¥ï¼Œç›´æ¥æ‹¼åˆ° pre
                pre += q

            elif isinstance(q, (tuple, list)):
                # éŸ³é¢‘è¾“å…¥ï¼š ("/path/to/audio.wav", )
                pre += f"Audio {audio_idx}: <audio>{q[0]}</audio>\n"
                audio_idx += 1

            elif isinstance(q, dict) and "image" in q:
                # å›¾åƒè¾“å…¥ï¼š {"image": "/path/to/image.png"}
                pre += f"Picture {image_idx}: <img>{q['image']}</img>\n"
                image_idx += 1

            else:
                # å…œåº•
                pre += str(q)

            # æŠŠ (pre, a) å­˜è¿›ä¸€ä¸ªfilteråˆ—è¡¨ï¼›è¿™æ ·ä¸‹ä¸€è½®æ‹¼æ¥çš„æ—¶å€™ï¼Œpreä¼šæ¸…ç©º
            history_filter.append((pre, a))
            pre = ""

        # 2) æ­¤æ—¶ history_filter[-1] å°±æ˜¯æœ€æ–°è¿™æ¡ (prompt, reply=None)
        #    model.chat() éœ€è¦æŠŠå†å²(é™¤æœ€åä¸€æ¡) ä½œä¸ºâ€œhistoryâ€ï¼Œæœ€åé‚£æ¡ prompt ä½œä¸ºâ€œmessageâ€
        #    å…·ä½“å†™æ³•å› ä½ å°è£…çš„ model.chat(...) è€Œå¼‚ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‹†åˆ†
        #    è¿™é‡Œæ¼”ç¤ºï¼šhistory = history_filter[:-1], message=history_filter[-1][0]
        history, message = history_filter[:-1], history_filter[-1][0]

        # ====== è°ƒç”¨å¤šæ¨¡æ€ chat æ¥å£ ======
        response, updated_history = model.chat(
            tokenizer, 
            message, 
            history=history
        )

        # ====== æ›´æ–°å‰ç«¯çš„æ˜¾ç¤º ======
        # 1) ç”Ÿæˆâ€œç”¨æˆ·ä¾§â€æ˜¾ç¤ºæ–‡å­—
        if isinstance(query, str):
            user_show = _parse_text(query)
        elif isinstance(query, (tuple, list)):
            user_show = f"[Audio File]: {query[0]}"
        elif isinstance(query, dict) and "image" in query:
            user_show = f"[Picture File]: {query['image']}"
        else:
            user_show = str(query)

        # 2) æ¨¡å‹å›å¤å¤§æ¦‚ç‡æ˜¯æ–‡æœ¬ï¼Œç›´æ¥ _parse_text
        _chatbot[-1] = (user_show, _parse_text(response))

        # ====== æ›´æ–° task_history ======
        task_history[-1] = (query, response)

        return _chatbot

    def regenerate(_chatbot, task_history):
        """
        å¦‚æœç”¨æˆ·ç‚¹äº† "Regenerate"ï¼Œåˆ™æŠŠæœ€åä¸€æ¬¡é—®ç­”å¯¹é‡æ–°ç”Ÿæˆ
        """
        if not task_history:
            return _chatbot
        item = task_history[-1]
        if item[1] is None:
            return _chatbot
        # æŠŠæœ€åä¸€ä¸ªå›ç­”ç½® None
        task_history[-1] = (item[0], None)
        chatbot_item = _chatbot.pop(-1)
        if chatbot_item[0] is None:
            _chatbot[-1] = (_chatbot[-1][0], None)
        else:
            _chatbot.append((chatbot_item[0], None))
        return predict(_chatbot, task_history)

    def add_text(history, task_history, text):
        """
        å¤„ç†æ–‡æœ¬è¾“å…¥
        """
        if not text.strip():
            return history, task_history, ""
        history = history + [(text, None)]
        task_history = task_history + [(text, None)]
        return history, task_history, ""

    def add_file(history, task_history, file):
        """
        å¤„ç†æ–‡ä»¶ä¸Šä¼ (å¦‚éŸ³é¢‘)
        """
        if file is None:
            return history, task_history
        history = history + [((file.name,), None)]
        task_history = task_history + [((file.name,), None)]
        return history, task_history

    def add_mic(history, task_history, file):
        """
        å¤„ç†éº¦å…‹é£å½•éŸ³
        """
        if file is None:
            return history, task_history
        os.rename(file, file + '.wav')
        history = history + [((file + '.wav',), None)]
        task_history = task_history + [((file + '.wav',), None)]
        return history, task_history

    def add_image(history, task_history, image):
        """
        å¤„ç†å›¾åƒä¸Šä¼ 
        """
        if image is None:
            return history, task_history
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        image_path = tempfile.NamedTemporaryFile(suffix=".png", delete=False).name
        Image.open(image).save(image_path)

        # ä»…ç”¨äºå‰ç«¯å±•ç¤º
        with open(image_path, "rb") as img_file:
            b64_string = base64.b64encode(img_file.read()).decode("utf-8")
        image_preview_html = f'<img src="data:image/png;base64,{b64_string}" alt="Picture" width="200"/>'

        # å‰ç«¯ï¼šå±•ç¤ºå›¾åƒ
        history = history + [(image_preview_html, None)]
        # åç«¯ï¼šå­˜æˆ (dict, None)ï¼Œdicté‡Œå¸¦ "image"
        task_history = task_history + [({"image": image_path}, None)]
        return history, task_history

    def reset_user_input():
        """æ¸…ç©ºè¾“å…¥æ¡†"""
        return gr.update(value="")

    def reset_state(task_history):
        """æ¸…ç©ºå…¨éƒ¨å†å²å¯¹è¯"""
        task_history.clear()
        return []

    with gr.Blocks() as demo:
        gr.Markdown("<h1 style='text-align:center'>Qwen-Audio-Chat Demo</h1>")
        
        chatbot = gr.Chatbot(label='Qwen-Audio-Chat', height=750, elem_id="chatbot")
        query = gr.Textbox(lines=2, label='Input')
        task_history = gr.State([])

        mic = gr.Audio(sources="microphone", type="filepath")
        addimage_btn = gr.UploadButton("ğŸ–¼ï¸ Upload Image", file_types=["image"])

        with gr.Row():
            empty_bin = gr.Button("ğŸ§¹ Clear History")
            submit_btn = gr.Button("ğŸš€ Submit")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate")
            addfile_btn = gr.UploadButton("ğŸ“ Upload File", file_types=["audio"])

        # ç»‘å®šäº‹ä»¶
        mic.change(add_mic, [chatbot, task_history, mic], [chatbot, task_history])
        addimage_btn.upload(add_image, [chatbot, task_history, addimage_btn], [chatbot, task_history])

        submit_btn.click(add_text, [chatbot, task_history, query], [chatbot, task_history, query])\
                  .then(predict, [chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(reset_user_input, [], [query])

        empty_bin.click(reset_state, [task_history], [chatbot], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)
        addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)

        gr.Markdown("""
        <small>
        Note: This is a multi-modal chat demo for Qwen. Please follow the usage license 
        and avoid generating harmful content.
        </small>
        """)

    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
    )


def main():
    args = _get_args()
    model, tokenizer = _load_model_tokenizer(args)
    _launch_demo(args, model, tokenizer)


if __name__ == '__main__':
    main()